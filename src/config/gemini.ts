import * as dotenv from "dotenv";
dotenv.config();

import { GoogleGenerativeAI } from "@google/generative-ai";

const apiKey = process.env.GEMINI_API_KEY!;
const genAI = new GoogleGenerativeAI(apiKey);

const model = genAI.getGenerativeModel({
  model: "gemini-2.0-flash",
});

const generationConfig = {
  temperature: 1,
  topP: 0.95,
  topK: 40,
  maxOutputTokens: 8192,
  responseMimeType: "application/json",
};

export const GenerateCourseLayout = model.startChat({
  generationConfig,
  history: [
    {
      role: "user",
      parts: [
        {
          text: "Generate a detailed course outline in JSON format based on the following specifications:\n\n- Category: [Programming]\n- Topic: [Python]\n- Additional Description: [Learn Python from scratch to become a ML Engineer]\n- Difficulty Level: [Advance]\n- Total Duration: [More than 3 hours]\n- Include Videos: [yes]\n- Number of Chapters: [20]",
        },
      ],
    },
    {
      role: "model",
      parts: [
        {
          text: '```json\n{\n  "courseTitle": "Python for Machine Learning Engineers: From Zero to Hero",\n  "category": "Programming",\n  "topic": "Python",\n  "description": "Learn Python from scratch to become a Machine Learning Engineer. This advanced course covers fundamental Python concepts and progresses to advanced topics essential for building robust and scalable machine learning models and applications.",\n  "difficulty": "Advanced",\n  "duration": "More than 3 hours",\n  "includesVideos": true,\n  "numberOfChapters": 20,\n  "courseOutline": [\n    {\n      "chapterNumber": 1,\n      "chapterTitle": "Python Setup and Environment Configuration",\n      "description": "Setting up Python, Anaconda, virtual environments, and IDE configuration for efficient machine learning development.",\n      "duration": "15 minutes",\n      "topicsCovered": [\n        "Installing Python (Anaconda Distribution)",\n        "Creating and managing virtual environments",\n        "IDE setup (VS Code, PyCharm)",\n        "Jupyter Notebook introduction and usage",\n        "Package management with pip and conda"\n      ],\n      "videoLink": "https://example.com/video/python-setup-1"\n    },\n    {\n      "chapterNumber": 2,\n      "chapterTitle": "Python Fundamentals Revisited: Data Types and Structures",\n      "description": "A rapid review of essential Python data types and structures, focusing on efficiency and best practices for machine learning.",\n      "duration": "20 minutes",\n      "topicsCovered": [\n        "Numeric types (int, float, complex)",\n        "Strings and string manipulation",\n        "Lists, tuples, dictionaries, and sets: advanced usage",\n        "List comprehensions and generator expressions",\n        "Memory management and efficiency considerations"\n      ],\n      "videoLink": "https://example.com/video/python-fundamentals-1"\n    },\n    {\n      "chapterNumber": 3,\n      "chapterTitle": "Functions and Modules: Building Reusable Code",\n      "description": "Advanced function techniques, including decorators, closures, and module creation for building modular and maintainable ML code.",\n      "duration": "20 minutes",\n      "topicsCovered": [\n        "Function definitions, arguments, and return values",\n        "Lambda functions and higher-order functions",\n        "Decorators for code enhancement and logging",\n        "Closures and non-local variables",\n        "Creating and using custom modules and packages"\n      ],\n      "videoLink": "https://example.com/video/python-functions-1"\n    },\n    {\n      "chapterNumber": 4,\n      "chapterTitle": "Object-Oriented Programming (OOP) in Python",\n      "description": "Deep dive into OOP principles, including inheritance, polymorphism, and encapsulation, with practical examples for ML project organization.",\n      "duration": "25 minutes",\n      "topicsCovered": [\n        "Classes and objects: creating and using them",\n        "Inheritance and polymorphism: building hierarchies",\n        "Encapsulation and data hiding",\n        "Abstract classes and interfaces",\n        "Design patterns for ML (e.g., Strategy, Factory)"\n      ],\n      "videoLink": "https://example.com/video/python-oop-1"\n    },\n    {\n      "chapterNumber": 5,\n      "chapterTitle": "File I/O and Data Serialization",\n      "description": "Reading and writing data to files, including CSV, JSON, and other formats, with a focus on efficient data handling for large datasets.",\n      "duration": "15 minutes",\n      "topicsCovered": [\n        "Reading and writing text files",\n        "Working with CSV files using the `csv` module",\n        "JSON serialization and deserialization",\n        "Handling binary data",\n        "Efficient data streaming for large datasets"\n      ],\n      "videoLink": "https://example.com/video/python-fileio-1"\n    },\n    {\n      "chapterNumber": 6,\n      "chapterTitle": "Error Handling and Debugging",\n      "description": "Robust error handling techniques and debugging strategies to ensure reliable and stable ML applications.",\n      "duration": "15 minutes",\n      "topicsCovered": [\n        "Try-except blocks and exception handling",\n        "Raising custom exceptions",\n        "Debugging with `pdb` and IDE debuggers",\n        "Logging and monitoring",\n        "Unit testing with `unittest`"\n      ],\n      "videoLink": "https://example.com/video/python-errors-1"\n    },\n    {\n      "chapterNumber": 7,\n      "chapterTitle": "Working with NumPy: Numerical Computing",\n      "description": "Advanced NumPy concepts, including broadcasting, vectorized operations, and linear algebra, essential for numerical computation in ML.",\n      "duration": "25 minutes",\n      "topicsCovered": [\n        "NumPy arrays: creation, indexing, and slicing",\n        "Broadcasting and vectorized operations",\n        "Linear algebra operations (matrix multiplication, decomposition)",\n        "Random number generation",\n        "Performance optimization with NumPy"\n      ],\n      "videoLink": "https://example.com/video/python-numpy-1"\n    },\n    {\n      "chapterNumber": 8,\n      "chapterTitle": "Pandas: Data Analysis and Manipulation",\n      "description": "Mastering Pandas DataFrames for data cleaning, transformation, and analysis, crucial for preparing data for ML models.",\n      "duration": "30 minutes",\n      "topicsCovered": [\n        "DataFrames: creation, indexing, and selection",\n        "Data cleaning and preprocessing (handling missing values, outliers)",\n        "Data transformation (grouping, pivoting, merging)",\n        "Time series analysis",\n        "Integration with other libraries (NumPy, scikit-learn)"\n      ],\n      "videoLink": "https://example.com/video/python-pandas-1"\n    },\n    {\n      "chapterNumber": 9,\n      "chapterTitle": "Matplotlib and Seaborn: Data Visualization",\n      "description": "Creating informative and visually appealing plots using Matplotlib and Seaborn to understand and communicate insights from data.",\n      "duration": "20 minutes",\n      "topicsCovered": [\n        "Basic plotting with Matplotlib",\n        "Customizing plots (labels, titles, legends)",\n        "Statistical visualizations with Seaborn",\n        "Interactive plots with Plotly or Bokeh",\n        "Choosing the right visualization for different data types"\n      ],\n      "videoLink": "https://example.com/video/python-vis-1"\n    },\n    {\n      "chapterNumber": 10,\n      "chapterTitle": "Scikit-learn: Machine Learning Algorithms",\n      "description": "Implementing and evaluating various machine learning algorithms using scikit-learn, including classification, regression, and clustering.",\n      "duration": "30 minutes",\n      "topicsCovered": [\n        "Supervised learning (classification, regression)",\n        "Unsupervised learning (clustering, dimensionality reduction)",\n        "Model selection and evaluation (cross-validation, metrics)",\n        "Pipeline construction",\n        "Hyperparameter tuning"\n      ],\n      "videoLink": "https://example.com/video/python-sklearn-1"\n    },\n    {\n      "chapterNumber": 11,\n      "chapterTitle": "Deep Learning with TensorFlow and Keras",\n      "description": "Building and training deep learning models using TensorFlow and Keras, including CNNs, RNNs, and LSTMs.",\n      "duration": "35 minutes",\n      "topicsCovered": [\n        "TensorFlow basics: tensors, operations, graphs",\n        "Keras API: building sequential and functional models",\n        "Convolutional Neural Networks (CNNs) for image recognition",\n        "Recurrent Neural Networks (RNNs) and LSTMs for sequence modeling",\n        "Training and evaluating deep learning models"\n      ],\n      "videoLink": "https://example.com/video/python-tf-1"\n    },\n    {\n      "chapterNumber": 12,\n      "chapterTitle": "Natural Language Processing (NLP) with NLTK and SpaCy",\n      "description": "Processing and analyzing text data using NLTK and SpaCy for tasks such as sentiment analysis, text classification, and named entity recognition.",\n      "duration": "25 minutes",\n      "topicsCovered": [\n        "Text preprocessing (tokenization, stemming, lemmatization)",\n        "Sentiment analysis",\n        "Text classification",\n        "Named entity recognition (NER)",\n        "Word embeddings (Word2Vec, GloVe)"\n      ],\n      "videoLink": "https://example.com/video/python-nlp-1"\n    },\n    {\n      "chapterNumber": 13,\n      "chapterTitle": "Computer Vision with OpenCV",\n      "description": "Working with images and videos using OpenCV for tasks such as object detection, image segmentation, and face recognition.",\n      "duration": "25 minutes",\n      "topicsCovered": [\n        "Image loading, processing, and display",\n        "Object detection (Haar cascades, YOLO, SSD)",\n        "Image segmentation",\n        "Feature extraction and matching",\n        "Face recognition"\n      ],\n      "videoLink": "https://example.com/video/python-cv-1"\n    },\n    {\n      "chapterNumber": 14,\n      "chapterTitle": "Web Scraping with Beautiful Soup and Scrapy",\n      "description": "Extracting data from websites using Beautiful Soup and Scrapy for building datasets and automating data collection.",\n      "duration": "20 minutes",\n      "topicsCovered": [\n        "HTML parsing with Beautiful Soup",\n        "Scrapy framework: building web crawlers",\n        "Handling dynamic content with Selenium",\n        "Data extraction and storage",\n        "Ethical considerations and best practices"\n      ],\n      "videoLink": "https://example.com/video/python-webscrape-1"\n    },\n    {\n      "chapterNumber": 15,\n      "chapterTitle": "APIs and Web Services",\n      "description": "Interacting with APIs and web services using the `requests` library for data retrieval and integration.",\n      "duration": "20 minutes",\n      "topicsCovered": [\n        "Making HTTP requests with `requests`",\n        "Working with JSON APIs",\n        "Authentication and authorization",\n        "Rate limiting and error handling",\n        "Building RESTful APIs with Flask or FastAPI (brief introduction)"\n      ],\n      "videoLink": "https://example.com/video/python-apis-1"\n    },\n    {\n      "chapterNumber": 16,\n      "chapterTitle": "Data Pipelines with Apache Airflow",\n      "description": "Designing and implementing data pipelines using Apache Airflow for automating data processing and model training workflows.",\n      "duration": "30 minutes",\n      "topicsCovered": [\n        "Airflow concepts: DAGs, tasks, operators",\n        "Setting up and configuring Airflow",\n        "Building and scheduling data pipelines",\n        "Monitoring and troubleshooting",\n        "Integration with other services (AWS, GCP, Azure)"\n      ],\n      "videoLink": "https://example.com/video/python-airflow-1"\n    },\n    {\n      "chapterNumber": 17,\n      "chapterTitle": "Cloud Computing for Machine Learning (AWS, GCP, Azure)",\n      "description": "Leveraging cloud computing platforms such as AWS, GCP, and Azure for deploying and scaling ML models and applications.",\n      "duration": "30 minutes",\n      "topicsCovered": [\n        "Introduction to cloud computing concepts",\n        "AWS (Amazon SageMaker, EC2, S3)",\n        "GCP (Google Cloud AI Platform, Compute Engine, Cloud Storage)",\n        "Azure (Azure Machine Learning, Virtual Machines, Blob Storage)",\n        "Deploying and scaling ML models in the cloud"\n      ],\n      "videoLink": "https://example.com/video/python-cloud-1"\n    },\n    {\n      "chapterNumber": 18,\n      "chapterTitle": "Model Deployment and Serving",\n      "description": "Deploying trained ML models using frameworks such as Flask, FastAPI, and Docker for real-time prediction serving.",\n      "duration": "25 minutes",\n      "topicsCovered": [\n        "Building RESTful APIs with Flask or FastAPI",\n        "Containerization with Docker",\n        "Model serving with TensorFlow Serving or TorchServe",\n        "Load balancing and scaling",\n        "Monitoring and logging"\n      ],\n      "videoLink": "https://example.com/video/python-deploy-1"\n    },\n    {\n      "chapterNumber": 19,\n      "chapterTitle": "Ethical Considerations in Machine Learning",\n      "description": "Addressing ethical concerns related to bias, fairness, and privacy in machine learning models and applications.",\n      "duration": "15 minutes",\n      "topicsCovered": [\n        "Bias in data and algorithms",\n        "Fairness metrics and mitigation techniques",\n        "Privacy-preserving techniques (differential privacy, federated learning)",\n        "Explainable AI (XAI)",\n        "Responsible AI development and deployment"\n      ],\n      "videoLink": "https://example.com/video/python-ethics-1"\n    },\n    {\n      "chapterNumber": 20,\n      "chapterTitle": "Advanced Python Libraries and Techniques",\n      "description": "Exploring advanced Python libraries and techniques for specialized ML tasks, such as graph databases, time series analysis, and optimization.",\n      "duration": "20 minutes",\n      "topicsCovered": [\n        "Graph databases with NetworkX or Neo4j",\n        "Advanced time series analysis with Statsmodels or Prophet",\n        "Optimization with SciPy or Pyomo",\n        "Parallel processing with multiprocessing or Dask",\n        "Advanced data structures (e.g., Bloom filters, Trie)"\n      ],\n      "videoLink": "https://example.com/video/python-advanced-1"\n    }\n  ]\n}\n```',
        },
      ],
    },
    {
      role: "user",
      parts: [
        {
          text: "Generate a detailed course outline in JSON format based on the following specifications. Make sure Enough details is provided for each and every topics that is included in the Course:\n\nCategory: [Programming]\n\nTopic: [Python]\n\nAdditional Description: [Learn Python from scratch to become a ML Engineer]\n\nDifficulty Level: [Beginner]\n\nTotal Duration: [More than 3 hours]\n\nInclude Videos: [yes]\n\nNumber of Chapters: [20]",
        },
      ],
    },
    {
      role: "model",
      parts: [
        {
          text: '```json\n{\n  "courseTitle": "Python for Machine Learning: A Beginner\'s Guide",\n  "category": "Programming",\n  "topic": "Python",\n  "description": "A comprehensive introduction to Python programming, tailored for aspiring Machine Learning Engineers. This course starts with the fundamentals and builds a strong foundation for future learning in Machine Learning.",\n  "difficulty": "Beginner",\n  "duration": "More than 3 hours",\n  "includesVideos": true,\n  "numberOfChapters": 20,\n  "courseOutline": [\n    {\n      "chapterNumber": 1,\n      "chapterTitle": "Introduction to Python and its Applications",\n      "description": "An overview of Python, its history, features, and diverse applications, including its pivotal role in Machine Learning.",\n      "duration": "10 minutes",\n      "topicsCovered": [\n        "What is Python?",\n        "History and Evolution of Python",\n        "Why Python for Machine Learning?",\n        "Python\'s Strengths and Weaknesses",\n        "Setting Expectations for the Course"\n      ],\n      "videoLink": "https://example.com/video/intro-python"\n    },\n    {\n      "chapterNumber": 2,\n      "chapterTitle": "Setting up Your Python Environment",\n      "description": "Guidance on installing Python and setting up a development environment using Anaconda and Jupyter Notebook.",\n      "duration": "15 minutes",\n      "topicsCovered": [\n        "Installing Python (Anaconda Distribution Recommended)",\n        "Understanding Anaconda Navigator",\n        "Creating and Managing Virtual Environments",\n        "Introduction to Jupyter Notebooks",\n        "Basic Jupyter Notebook Operations (Cells, Saving, etc.)"\n      ],\n      "videoLink": "https://example.com/video/setup-env"\n    },\n    {\n      "chapterNumber": 3,\n      "chapterTitle": "Python Syntax and Basic Data Types",\n      "description": "Understanding fundamental Python syntax, including variables, data types (integers, floats, strings, booleans), and basic operations.",\n      "duration": "20 minutes",\n      "topicsCovered": [\n        "Variables and Assignment",\n        "Integers, Floats, and Basic Arithmetic Operations",\n        "Strings: Definition, Concatenation, and Basic Methods",\n        "Booleans: True and False Values",\n        "Comments in Python"\n      ],\n      "videoLink": "https://example.com/video/syntax-datatypes"\n    },\n    {\n      "chapterNumber": 4,\n      "chapterTitle": "Operators in Python",\n      "description": "Exploring different types of operators in Python, including arithmetic, comparison, logical, and assignment operators.",\n      "duration": "15 minutes",\n      "topicsCovered": [\n        "Arithmetic Operators (+, -, *, /, %, //, **)",\n        "Comparison Operators (==, !=, >, <, >=, <=)",\n        "Logical Operators (and, or, not)",\n        "Assignment Operators (=, +=, -=, *=, /=, etc.)",\n        "Operator Precedence"\n      ],\n      "videoLink": "https://example.com/video/operators"\n    },\n    {\n      "chapterNumber": 5,\n      "chapterTitle": "Control Flow: If Statements",\n      "description": "Learning how to control the flow of your program using if, elif, and else statements.",\n      "duration": "20 minutes",\n      "topicsCovered": [\n        "Basic `if` Statement",\n        "The `else` Clause",\n        "Using `elif` (else if)",\n        "Nested `if` Statements",\n        "Real-world Examples"\n      ],\n      "videoLink": "https://example.com/video/if-statements"\n    },\n    {\n      "chapterNumber": 6,\n      "chapterTitle": "Control Flow: Loops (For and While)",\n      "description": "Understanding and using for and while loops to iterate over sequences and execute code repeatedly.",\n      "duration": "25 minutes",\n      "topicsCovered": [\n        "The `for` Loop: Iterating over Sequences",\n        "The `range()` Function",\n        "The `while` Loop: Conditional Execution",\n        "`break` and `continue` Statements",\n        "Nested Loops"\n      ],\n      "videoLink": "https://example.com/video/loops"\n    },\n    {\n      "chapterNumber": 7,\n      "chapterTitle": "Data Structures: Lists",\n      "description": "Introducing lists, a fundamental data structure for storing ordered collections of items.",\n      "duration": "20 minutes",\n      "topicsCovered": [\n        "Creating and Accessing Lists",\n        "List Methods (append, insert, remove, pop, etc.)",\n        "List Slicing",\n        "List Comprehensions (Introduction)",\n        "Common List Operations (len, max, min, sum)"\n      ],\n      "videoLink": "https://example.com/video/lists"\n    },\n    {\n      "chapterNumber": 8,\n      "chapterTitle": "Data Structures: Tuples",\n      "description": "Understanding tuples, an immutable data structure similar to lists.",\n      "duration": "15 minutes",\n      "topicsCovered": [\n        "Creating and Accessing Tuples",\n        "Tuple Immutability",\n        "When to Use Tuples vs. Lists",\n        "Tuple Packing and Unpacking",\n        "Basic Tuple Methods (count, index)"\n      ],\n      "videoLink": "https://example.com/video/tuples"\n    },\n    {\n      "chapterNumber": 9,\n      "chapterTitle": "Data Structures: Dictionaries",\n      "description": "Learning about dictionaries, a key-value data structure for storing and retrieving data efficiently.",\n      "duration": "25 minutes",\n      "topicsCovered": [\n        "Creating and Accessing Dictionaries",\n        "Dictionary Methods (get, keys, values, items, update, etc.)",\n        "Adding, Modifying, and Deleting Dictionary Entries",\n        "Dictionary Comprehensions (Introduction)",\n        "Using Dictionaries for Data Storage and Retrieval"\n      ],\n      "videoLink": "https://example.com/video/dictionaries"\n    },\n    {\n      "chapterNumber": 10,\n      "chapterTitle": "Data Structures: Sets",\n      "description": "Exploring sets, an unordered collection of unique elements.",\n      "duration": "15 minutes",\n      "topicsCovered": [\n        "Creating and Accessing Sets",\n        "Set Operations (union, intersection, difference, etc.)",\n        "Adding and Removing Elements from a Set",\n        "Set Comprehensions (Introduction)",\n        "When to Use Sets"\n      ],\n      "videoLink": "https://example.com/video/sets"\n    },\n    {\n      "chapterNumber": 11,\n      "chapterTitle": "Functions: Defining and Calling",\n      "description": "Learning how to define and call functions to create reusable blocks of code.",\n      "duration": "20 minutes",\n      "topicsCovered": [\n        "Defining Functions with `def`",\n        "Function Arguments (Positional, Keyword, Default)",\n        "Returning Values from a Function",\n        "Function Scope",\n        "Docstrings"\n      ],\n      "videoLink": "https://example.com/video/functions"\n    },\n    {\n      "chapterNumber": 12,\n      "chapterTitle": "Modules and Packages",\n      "description": "Understanding how to use modules and packages to organize and reuse code.",\n      "duration": "20 minutes",\n      "topicsCovered": [\n        "Importing Modules (`import`, `from ... import ...`, `as`)",\n        "Built-in Modules (math, random, datetime)",\n        "Creating Your Own Modules",\n        "Introduction to Packages",\n        "Installing Packages with `pip`"\n      ],\n      "videoLink": "https://example.com/video/modules"\n    },\n    {\n      "chapterNumber": 13,\n      "chapterTitle": "Working with Files",\n      "description": "Learning how to read and write data to files.",\n      "duration": "25 minutes",\n      "topicsCovered": [\n        "Opening and Closing Files (`open()`, `close()`)",\n        "Reading from Files (`read()`, `readline()`, `readlines()`)",\n        "Writing to Files (`write()`, `writelines()`)",\n        "File Modes (read, write, append)",\n        "Using `with` statement for Automatic File Closing"\n      ],\n      "videoLink": "https://example.com/video/files"\n    },\n    {\n      "chapterNumber": 14,\n      "chapterTitle": "Error Handling with Try-Except",\n      "description": "Implementing error handling to prevent program crashes and handle exceptions gracefully.",\n      "duration": "15 minutes",\n      "topicsCovered": [\n        "Understanding Exceptions",\n        "The `try-except` Block",\n        "Handling Specific Exceptions",\n        "The `finally` Clause",\n        "Raising Exceptions"\n      ],\n      "videoLink": "https://example.com/video/error-handling"\n    },\n    {\n      "chapterNumber": 15,\n      "chapterTitle": "Introduction to NumPy: Arrays and Basic Operations",\n      "description": "A gentle introduction to NumPy, a fundamental library for numerical computing in Python, focusing on arrays and basic operations.",\n      "duration": "30 minutes",\n      "topicsCovered": [\n        "What is NumPy?",\n        "Creating NumPy Arrays",\n        "Array Attributes (shape, dtype, size)",\n        "Array Indexing and Slicing",\n        "Basic Array Operations (+, -, *, /, etc.)"\n      ],\n      "videoLink": "https://example.com/video/numpy-intro"\n    },\n    {\n      "chapterNumber": 16,\n      "chapterTitle": "Introduction to Pandas: DataFrames",\n      "description": "An introduction to Pandas, a library for data analysis and manipulation, focusing on DataFrames.",\n      "duration": "30 minutes",\n      "topicsCovered": [\n        "What is Pandas?",\n        "Creating DataFrames from Dictionaries, Lists, and NumPy Arrays",\n        "DataFrame Attributes (index, columns, dtypes)",\n        "DataFrame Indexing and Selection",\n        "Basic DataFrame Operations (head, tail, describe, info)"\n      ],\n      "videoLink": "https://example.com/video/pandas-intro"\n    },\n    {\n      "chapterNumber": 17,\n      "chapterTitle": "Introduction to Matplotlib: Basic Plotting",\n      "description": "A basic introduction to Matplotlib, a library for creating static, interactive, and animated visualizations in Python.",\n      "duration": "20 minutes",\n      "topicsCovered": [\n        "What is Matplotlib?",\n        "Creating Basic Plots (line, scatter, bar)",\n        "Adding Labels and Titles",\n        "Customizing Plot Appearance (colors, markers, line styles)",\n        "Saving Plots to Files"\n      ],\n      "videoLink": "https://example.com/video/matplotlib-intro"\n    },\n    {\n      "chapterNumber": 18,\n      "chapterTitle": "Introduction to Scikit-learn: First Model",\n      "description": "Your first machine learning model with Scikit-learn: a simple linear regression example.",\n      "duration": "25 minutes",\n      "topicsCovered": [\n        "What is Scikit-learn?",\n        "Preparing Data for Modeling",\n        "Training a Linear Regression Model",\n        "Making Predictions",\n        "Evaluating Model Performance (Introduction)"\n      ],\n      "videoLink": "https://example.com/video/sklearn-first-model"\n    },\n    {\n      "chapterNumber": 19,\n      "chapterTitle": "Next Steps in Machine Learning",\n      "description": "Guidance on where to go next in your Machine Learning journey, including recommended resources and topics to explore.",\n      "duration": "10 minutes",\n      "topicsCovered": [\n        "Recommended Resources for Further Learning",\n        "Key Concepts to Explore (Classification, Clustering, Deep Learning)",\n        "Joining the Machine Learning Community",\n        "Building Your Portfolio"\n      ],\n      "videoLink": "https://example.com/video/next-steps"\n    },\n    {\n      "chapterNumber": 20,\n      "chapterTitle": "Project: Building a Simple Calculator",\n      "description": "A hands-on project to solidify your understanding of Python fundamentals by building a simple calculator.",\n      "duration": "30 minutes",\n      "topicsCovered": [\n        "Project Overview",\n        "Planning the Calculator\'s Functionality",\n        "Implementing the Calculator using Functions, Loops, and Conditional Statements",\n        "Testing and Debugging",\n        "Adding Features and Enhancements"\n      ],\n      "videoLink": "https://example.com/video/calculator-project"\n    }\n  ]\n}\n```',
        },
      ],
    },
    {
      role: "user",
      parts: [
        {
          text: "I am using the free tier which has RPM of 15 and 1500 per day. What should I limit in the prompt to overloading my api? For example if user wants to generate 200 chapters, that would be kind of too much right? Tell me exactly what to limit to stay under the RPM?\n",
        },
      ],
    },
    {
      role: "model",
      parts: [
        {
          text: 'Okay, here\'s a breakdown of how to limit your prompts and API usage to stay within your free tier limits (15 RPM and 1500 requests per day), specifically regarding generating course outlines:\n\n**Understanding the Limits**\n\n*   **RPM (Requests Per Minute):** You can make a maximum of 15 API calls within any given minute.  Exceeding this will result in rate limiting (your requests being rejected).\n*   **Daily Limit:** You have a total of 1500 requests available each day.\n\n**Key Considerations for Limiting Prompts**\n\n1.  **Chapter Generation is Expensive:** Generating even *one* detailed chapter outline can easily consume several API requests, especially if you\'re asking for descriptions, topics, and video links within each chapter. Generating 200 chapters is almost certainly *way* over your RPM *and* daily limit.\n\n2.  **Focus on Efficiency:**  Structure your prompt to get the most information with the fewest API calls.  Batch related requests whenever possible.\n\n3.  **Plan for Error Handling:** Include error handling in your code to gracefully deal with rate limits and other potential issues.  This is crucial for providing a good user experience even within the free tier.\n\n**Specific Prompt Limits and Strategies**\n\nHere\'s what you should limit and how to do it, with code examples:\n\n*   **a.  Limit the Number of Chapters:**  This is the MOST important thing to control.  Don\'t let users request an unlimited number of chapters.\n    *   **Example:** Set a maximum allowable chapter count in your UI/code.\n\n    ```python\n    MAX_CHAPTERS = 10  # Example: Limit to 10 chapters\n\n    def generate_outline(user_requested_chapters):\n      num_chapters = min(user_requested_chapters, MAX_CHAPTERS)\n      print(f"Generating outline for {num_chapters} chapters (max allowed: {MAX_CHAPTERS})")\n      # ... your API call logic here ...\n    ```\n\n*   **b.  Limit the Level of Detail:** Requesting a simpler outline consumes fewer tokens and potentially fewer API calls.\n\n    *   **Example:**  Offer different outline levels (e.g., "Basic", "Detailed", "Very Detailed") and adjust the prompt accordingly.  A "Basic" outline might just have the chapter title and a one-sentence description.  A "Detailed" outline could include topic lists and video link suggestions.  The "Very Detailed" outline is what we have been using.\n\n    ```python\n    def generate_prompt(topic, num_chapters, detail_level="Basic"):\n      if detail_level == "Basic":\n        prompt = f"Generate a course outline for {topic} with {num_chapters} chapters. Just provide the chapter title and a one-sentence description for each."\n      elif detail_level == "Detailed":\n        prompt = f"Generate a course outline for {topic} with {num_chapters} chapters. Provide the chapter title, a longer detailed description, and a list of 3-5 relevant topics for each chapter."\n      elif detail_level == "Very Detailed":\n                prompt = f"Generate a detailed course outline in JSON format based on the following specifications:\\nCategory: [Programming]\\nTopic: [{topic}]\\nAdditional Description: [Learn Python from scratch to become a ML Engineer]\\nDifficulty Level: [Beginner]\\nTotal Duration: [More than 3 hours]\\nInclude Videos: [yes]\\nNumber of Chapters: [{num_chapters}]"\n\n      return prompt\n    ```\n\n*   **c.  Implement Rate Limiting (Even with the Input Limits):** This is a safety net in case of unexpected code behavior or user input errors.  Many libraries exist to help with this.\n    *   **Example (using `time.sleep` - basic):**\n\n    ```python\n    import time\n\n    REQUESTS_PER_MINUTE = 15\n    SLEEP_TIME = 60 / REQUESTS_PER_MINUTE #Sleep time in seconds\n    api_call_count = 0\n    last_minute_start = time.time()\n\n\n    def make_api_call(prompt): #Replace this with your actual api call function\n        global api_call_count, last_minute_start\n        current_time = time.time()\n\n        if current_time - last_minute_start >= 60: #Check how long since last minute\n            api_call_count = 0\n            last_minute_start = current_time\n\n        if api_call_count >= REQUESTS_PER_MINUTE:\n            sleep_duration = 60 - (current_time - last_minute_start)  #Seconds until the next minute\n            print(f"Rate limit reached. Sleeping for {sleep_duration:.2f} seconds...")\n            time.sleep(sleep_duration) #Sleep until the next minute resets\n            api_call_count = 0\n            last_minute_start = time.time()\n\n\n        time.sleep(SLEEP_TIME)\n        api_call_count += 1\n\n        # REPLACE THIS with your actual API call\n        print(f"Making API call: {prompt}")  # Simulate API call\n        return f"Response for: {prompt}"\n    ```\n\n*   **d. Implement a queue:** Instead of generating the full course immediately, you could add sections to a queue. The queue is processed in the background, respecting your RPM. This provides a smoother user experience as they don\'t have to wait for everything immediately. This requires more complex coding.\n\n**Putting it all together (Conceptual Example):**\n\n1.  **UI:** The user selects a topic and the number of chapters (limited to, say, 10). They also select the detail level ("Basic", "Detailed").\n2.  **Code:**\n    *   Your code uses the selected options to generate a prompt (using the `generate_prompt` function example above).\n    *   You then use `make_api_call` (or your actual API call function, wrapped with rate limiting) to get the chapter outline.\n    *   Handle the API response, parse the JSON, and display the results to the user.\n3.  **Error Handling:**  If the API returns an error (e.g., rate limit exceeded), display a friendly message to the user, explaining that they need to wait before generating more content.\n\n**Key Takeaways**\n\n*   **Prioritize limiting the number of chapters.** It\'s the biggest resource hog.\n*   **Implement rate limiting in your code** (using `time.sleep` or a more robust library).\n*   **Offer different detail levels** to give users some control over API usage.\n*   **Handle errors gracefully** and provide informative messages to the user.\n\nBy carefully controlling these factors, you can effectively use the free tier and avoid overloading your API. Remember to test your code thoroughly with different settings to ensure that you stay within your limits. Good luck!\n',
        },
      ],
    },
    {
      role: "user",
      parts: [
        {
          text: "Generate a streamlined yet robust course outline in JSON format based on the following specifications:\n\nCategory: [Programming]\n\nTopic: [Python]\n\nAdditional Description: [Learn Python from scratch to become a ML Engineer]\n\nDifficulty Level: [Advance]\n\nTotal Duration: [More than 3 hours]\n\nInclude Videos: [yes]\n\nNumber of Chapters: [20]",
        },
      ],
    },
    {
      role: "model",
      parts: [
        {
          text: '```json\n{\n  "courseTitle": "Advanced Python for Machine Learning Engineers",\n  "category": "Programming",\n  "topic": "Python",\n  "description": "Master advanced Python concepts and libraries essential for building and deploying scalable machine learning solutions. Focuses on efficiency, best practices, and real-world applications.",\n  "difficulty": "Advanced",\n  "duration": "More than 3 hours",\n  "includesVideos": true,\n  "numberOfChapters": 20,\n  "courseOutline": [\n    {\n      "chapterNumber": 1,\n      "chapterTitle": "Python Optimization & Profiling",\n      "description": "Techniques for identifying and addressing performance bottlenecks in Python code. Covers profiling tools and optimization strategies.",\n      "duration": "15 minutes",\n      "topicsCovered": ["Profiling with `cProfile` and `line_profiler`", "Memory management optimization", "Cython integration", "Vectorization strategies", "Benchmarking techniques"],\n      "videoLink": "https://example.com/video/opt-prof"\n    },\n    {\n      "chapterNumber": 2,\n      "chapterTitle": "Advanced Data Structures & Algorithms",\n      "description": "Implementation and application of advanced data structures and algorithms relevant to machine learning.",\n      "duration": "20 minutes",\n      "topicsCovered": ["Heaps and priority queues", "Graph algorithms (BFS, DFS)", "Trees (balanced trees, tries)", "Dynamic programming techniques", "Bloom filters"],\n      "videoLink": "https://example.com/video/adv-ds-alg"\n    },\n    {\n      "chapterNumber": 3,\n      "chapterTitle": "Concurrency & Parallelism",\n      "description": "Utilizing threading, multiprocessing, and asynchronous programming to improve the performance of computationally intensive ML tasks.",\n      "duration": "20 minutes",\n      "topicsCovered": ["Threading vs. multiprocessing", "Asynchronous programming with `asyncio`", "Process pools and thread pools", "Global Interpreter Lock (GIL) considerations", "Distributed computing frameworks (Dask introduction)"],\n      "videoLink": "https://example.com/video/concurrency"\n    },\n    {\n      "chapterNumber": 4,\n      "chapterTitle": "Metaclasses & Dynamic Code Generation",\n      "description": "Understanding and applying metaclasses and dynamic code generation for advanced customization and flexibility in ML frameworks.",\n      "duration": "20 minutes",\n      "topicsCovered": ["Metaclass concepts and creation", "Dynamic class and object creation", "Code generation with `exec` and `eval`", "Use cases in ML model configuration", "Monkey patching"],\n      "videoLink": "https://example.com/video/metaclasses"\n    },\n    {\n      "chapterNumber": 5,\n      "chapterTitle": "Descriptor Protocol",\n      "description": "Understanding and implementing the descriptor protocol for managing attribute access and validation.",\n      "duration": "15 minutes",\n      "topicsCovered": ["Understanding descriptors (`__get__`, `__set__`, `__delete__`)", "Property decorators", "Data validation with descriptors", "Non-data descriptors", "Practical examples in model parameter management"],\n      "videoLink": "https://example.com/video/descriptor"\n    },\n    {\n      "chapterNumber": 6,\n      "chapterTitle": "Memory Management & Garbage Collection",\n      "description": "In-depth exploration of Python\'s memory management and garbage collection mechanisms to optimize memory usage in large-scale ML projects.",\n      "duration": "15 minutes",\n      "topicsCovered": ["Python\'s memory model", "Garbage collection algorithms", "Memory profiling tools", "Circular references and weak references", "Techniques for reducing memory footprint"],\n      "videoLink": "https://example.com/video/memory-mgmt"\n    },\n    {\n      "chapterNumber": 7,\n      "chapterTitle": "Advanced NumPy: Broadcasting & Ufuncs",\n      "description": "Deep dive into NumPy\'s broadcasting rules and universal functions (ufuncs) for efficient array manipulation and computation.",\n      "duration": "25 minutes",\n      "topicsCovered": ["Broadcasting rules and applications", "Creating custom ufuncs", "Generalized ufuncs", "Memory views and strides", "Performance optimization with NumPy"],\n      "videoLink": "https://example.com/video/adv-numpy"\n    },\n    {\n      "chapterNumber": 8,\n      "chapterTitle": "Advanced Pandas: Custom Accessors & Performance",\n      "description": "Extending Pandas DataFrames with custom accessors and optimizing performance for large datasets.",\n      "duration": "25 minutes",\n      "topicsCovered": ["Creating custom accessors", "Categorical data types", "Sparse data structures", "Optimizing DataFrame operations", "Out-of-core data processing with Dask"],\n      "videoLink": "https://example.com/video/adv-pandas"\n    },\n    {\n      "chapterNumber": 9,\n      "chapterTitle": "Custom Scikit-learn Transformers & Estimators",\n      "description": "Building custom transformers and estimators in scikit-learn to extend its functionality and tailor it to specific ML problems.",\n      "duration": "25 minutes",\n      "topicsCovered": ["Creating custom transformers (fit, transform)", "Creating custom estimators (fit, predict)", "Integrating custom components into pipelines", "Model validation and testing", "Composing complex pipelines with custom components"],\n      "videoLink": "https://example.com/video/custom-sklearn"\n    },\n    {\n      "chapterNumber": 10,\n      "chapterTitle": "TensorFlow Internals & Custom Layers",\n      "description": "Understanding the inner workings of TensorFlow and building custom layers for advanced deep learning architectures.",\n      "duration": "30 minutes",\n      "topicsCovered": ["TensorFlow graph execution", "Custom layer creation (Keras API)", "Custom loss functions and metrics", "Gradient tape and custom training loops", "TensorBoard integration"],\n      "videoLink": "https://example.com/video/tf-internals"\n    },\n    {\n      "chapterNumber": 11,\n      "chapterTitle": "PyTorch Internals & Custom Autograd Functions",\n      "description": "Understanding the inner workings of PyTorch and building custom autograd functions for advanced deep learning architectures.",\n      "duration": "30 minutes",\n      "topicsCovered": ["PyTorch dynamic graph execution", "Custom autograd function creation", "TorchScript and model deployment", "Custom data loaders and datasets", "Profiling and optimization"],\n      "videoLink": "https://example.com/video/pytorch-internals"\n    },\n    {\n      "chapterNumber": 12,\n      "chapterTitle": "Probabilistic Programming with PyMC3",\n      "description": "Introduction to probabilistic programming with PyMC3 for Bayesian modeling and inference.",\n      "duration": "20 minutes",\n      "topicsCovered": ["Bayesian modeling concepts", "Building models with PyMC3", "Markov Chain Monte Carlo (MCMC) sampling", "Model diagnostics and evaluation", "Hierarchical modeling"],\n      "videoLink": "https://example.com/video/pymc3"\n    },\n    {\n      "chapterNumber": 13,\n      "chapterTitle": "Graph Neural Networks with DGL",\n      "description": "Building and training graph neural networks (GNNs) using the Deep Graph Library (DGL).",\n      "duration": "25 minutes",\n      "topicsCovered": ["Graph data structures and representations", "DGL API overview", "Implementing GNN layers (GraphConv, GAT)", "Node classification and link prediction tasks", "Scalable GNN training techniques"],\n      "videoLink": "https://example.com/video/dgl"\n    },\n    {\n      "chapterNumber": 14,\n      "chapterTitle": "Explainable AI (XAI) with SHAP & LIME",\n      "description": "Techniques for explaining the predictions of machine learning models using SHAP and LIME.",\n      "duration": "20 minutes",\n      "topicsCovered": ["SHAP (SHapley Additive exPlanations) values", "LIME (Local Interpretable Model-agnostic Explanations)", "Global and local explanations", "Visualizing and interpreting explanations", "Ethical considerations"],\n      "videoLink": "https://example.com/video/xai"\n    },\n    {\n      "chapterNumber": 15,\n      "chapterTitle": "MLOps: Model Deployment & Monitoring",\n      "description": "Best practices for deploying, monitoring, and maintaining machine learning models in production.",\n      "duration": "25 minutes",\n      "topicsCovered": ["Model serving frameworks (TensorFlow Serving, TorchServe, FastAPI)", "Containerization with Docker", "Orchestration with Kubernetes", "Model monitoring and alerting", "Continuous integration and continuous deployment (CI/CD)"],\n      "videoLink": "https://example.com/video/mlops"\n    },\n    {\n      "chapterNumber": 16,\n      "chapterTitle": "Federated Learning",\n      "description": "Implementing federated learning for training models on decentralized data while preserving privacy.",\n      "duration": "25 minutes",\n      "topicsCovered": ["Federated learning concepts and algorithms", "Secure aggregation techniques", "Differential privacy for federated learning", "Client selection and communication strategies", "Practical implementations with frameworks like Flower or FedScale"],\n      "videoLink": "https://example.com/video/federated-learning"\n    },\n    {\n      "chapterNumber": 17,\n      "chapterTitle": "Reinforcement Learning",\n      "description": "An introduction to Reinforcement Learning concepts and how to implement them in Python.",\n      "duration": "30 minutes",\n      "topicsCovered": ["Markov Decision Process", "Q-Learning", "Deep Q-Networks (DQN)", "Policy Gradients", "Actor-Critic Methods"],\n      "videoLink": "https://example.com/video/reinforcement"\n    },\n    {\n      "chapterNumber": 18,\n      "chapterTitle": "Time Series Analysis with Advanced Methods",\n      "description": "Advanced techniques for analyzing and forecasting time series data, including state space models and deep learning approaches.",\n      "duration": "25 minutes",\n      "topicsCovered": ["State space models (Kalman filter, Hidden Markov Models)", "Deep learning for time series forecasting (LSTMs, Transformers)", "Causal inference in time series data", "Anomaly detection and change point detection", "Evaluation metrics for time series forecasting"],\n      "videoLink": "https://example.com/video/timeseries-adv"\n    },\n    {\n      "chapterNumber": 19,\n      "chapterTitle": "Generative Adversarial Networks (GANs)",\n      "description": "Building and training Generative Adversarial Networks (GANs) for generating realistic synthetic data.",\n      "duration": "30 minutes",\n      "topicsCovered": ["GAN architectures (DCGAN, WGAN, StyleGAN)", "Training GANs and addressing mode collapse", "Conditional GANs", "Applications of GANs in image generation, data augmentation, and anomaly detection", "Evaluation metrics for GANs"],\n      "videoLink": "https://example.com/video/gans"\n    },\n    {\n      "chapterNumber": 20,\n      "chapterTitle": "Ethical Considerations in Advanced ML",\n      "description": "Addressing ethical considerations in advanced machine learning, including bias, fairness, privacy, and transparency.",\n      "duration": "15 minutes",\n      "topicsCovered": ["Bias detection and mitigation techniques", "Fairness metrics and algorithms", "Privacy-preserving techniques (differential privacy, federated learning)", "Explainable AI (XAI) and interpretability", "Responsible AI development and deployment"],\n      "videoLink": "https://example.com/video/ethics-adv"\n    }\n  ]\n}\n```\n',
        },
      ],
    },
  ],
});

export const GenerateContentChat = model.startChat({
  generationConfig,
  history: [
    {
      role: "user",
      parts: [
        {
          text: "Generate detailed content in JSON format for Chapter [1]: '[Python Optimization & Profiling]'. This chapter is part of an advanced course on Python for machine learning engineers, focusing on efficiency, best practices, and real-world applications.\n\nThe content should be structured as follows:\n\nIntroduction: Provide a brief overview of the chapter's topics and their relevance to machine learning engineering.\n\nTopic Sections: For each of the following topics, offer an in-depth explanation, including key concepts, techniques, and best practices. Use examples, code snippets, and practical applications to illustrate the points.\n\n[Profiling with cProfile and line_profiler]\n\n[Memory management optimization]\n\n[Cython integration]\n\n[Vectorization strategies]\n\n[Benchmarking techniques]\n\nConclusion: Summarize the main takeaways from the chapter and how they contribute to building scalable and efficient machine learning solutions.\n\nGuidelines:\n\nTailor the content to an advanced audience with a strong foundation in Python and machine learning.\n\nEnsure all code snippets are accurate, functional, and properly formatted using markdown code blocks (python ... ) for proper rendering in a UI.\n\nInclude comments within code snippets to explain complex sections.\n\nHighlight real-world applications and efficiency considerations relevant to machine learning where applicable.\n\nAim for clarity and depth, providing insights that go beyond basic tutorials.\n\nKeep the content comprehensive yet concise, suitable for a chapter in a course with up to 20 chapters.",
        },
      ],
    },
    {
      role: "model",
      parts: [
        {
          text: '```json\n{\n  "chapter": {\n    "number": 1,\n    "title": "[Python Optimization & Profiling]",\n    "content": {\n      "introduction": "Welcome to the first chapter of this advanced Python course for Machine Learning Engineers.  This chapter focuses on Python optimization and profiling techniques essential for building efficient and scalable machine learning pipelines. In this chapter, we\'ll move beyond basic Python and delve into strategies for identifying performance bottlenecks, optimizing memory usage, leveraging Cython for performance-critical sections, employing vectorization techniques with libraries like NumPy, and accurately benchmarking our code to validate improvements. Understanding these techniques is crucial for deploying models at scale, handling large datasets, and minimizing computational costs in real-world machine learning applications.",\n      "topics": [\n        {\n          "title": "[Profiling with cProfile and line_profiler]",\n          "content": "Profiling is the process of analyzing your code\'s execution to identify areas that consume the most time or resources. This allows you to focus your optimization efforts on the most impactful bottlenecks. We\'ll explore two powerful profiling tools: `cProfile` and `line_profiler`.",\n          "subtopics": [\n            {\n              "title": "`cProfile`",\n              "content": "`cProfile` is Python\'s built-in profiler, implemented in C, providing a low-overhead way to profile your code. It provides function-level timings, helping you identify which functions are taking the most time.\\n\\n**Key Concepts:**\\n*   **Function Call Counts:**  How many times each function is called.\\n*   **Total Time:** The total time spent in each function, including its callees.\\n*   **Self Time:** The total time spent in each function, excluding its callees.  This is typically the most relevant metric for finding bottlenecks.\\n\\n**Usage:**\\n\\n```python\\nimport cProfile\\nimport pstats\\nimport io\\n\\n# Sample function to profile\\ndef slow_function():\\n    result = 0\\n    for i in range(1000000):\\n        result += i\\n    return result\\n\\ndef another_slow_function():\\n    result = 0\\n    for i in range(500000):\\n        result *= (i + 1) if (i+1) % 2 == 0 else 1 # Simulate different complexity\\n    return result\\n\\ndef main_function():\\n    slow_function()\\n    another_slow_function()\\n\\n# Profile the code\\npr = cProfile.Profile()\\npr.enable()\\nmain_function()\\npr.disable()\\n\\n# Print the results\\ns = io.StringIO()\\nsortby = \'cumulative\'\\nps = pstats.Stats(pr, stream=s).sort_stats(sortby)\\nps.print_stats()\\n\\nprint(s.getvalue())\\n\\n# Alternatively, run from the command line:\\n# python -m cProfile my_script.py > profile_output.txt\\n```\\n\\n**Explanation:**\\n1.  We import `cProfile`, `pstats`, and `io` modules.\\n2.  We define a function `slow_function` that simulates a computationally intensive task.\\n3.  We create a `cProfile.Profile` object.\\n4.  We enable the profiler using `pr.enable()`.\\n5.  We execute the code to be profiled (`main_function()` in this example).\\n6.  We disable the profiler using `pr.disable()`.\\n7.  We use `pstats.Stats` to analyze the profiling data and sort it by cumulative time.\\n8.  We print the profiling results to the console. The sorted results show the total time spent in each function and its callees, ordered from longest to shortest.\\n\\n**Real-World Application:**\\nImagine you\'re training a complex machine learning model and noticing slow training times. Using `cProfile`, you can identify which parts of your data preprocessing pipeline or model training loop are taking the most time.  You might discover that a specific data transformation function or a particular layer in your neural network is the bottleneck.\\n\\n**Best Practices:**\\n*   Run `cProfile` on a representative workload to accurately capture performance characteristics.\\n*   Focus on functions with high self-time, as these are the most promising targets for optimization.\\n*   Use the `sortby` argument in `pstats.Stats` to sort the profiling results by different criteria (e.g., \'time\', \'cumulative\')."\n            },\n            {\n              "title": "`line_profiler`",\n              "content": "`line_profiler` provides line-by-line profiling, allowing you to pinpoint the exact lines of code that are consuming the most time.  This is particularly useful for optimizing complex functions where `cProfile`\'s function-level profiling isn\'t granular enough.\\n\\n**Key Concepts:**\\n*   **Line-by-Line Timings:**  Detailed timings for each line of code within a function.\\n*   **Hits:**  Number of times each line of code is executed.\\n*   **Time per Hit:** Average time spent executing each line of code.\\n\\n**Usage:**\\n\\n```python\\n# Install line_profiler: pip install line_profiler\\n# Install psutil: pip install psutil (required for some line_profiler features)\\n\\nimport numpy as np\\n\\n@profile  # Decorator to indicate the function to be profiled\\ndef process_data(data):\\n    # Simulate some data processing tasks\\n    results = []\\n    for row in data:\\n        processed_row = np.sqrt(row) # NumPy\'s sqrt is optimized\\n        results.append(processed_row)\\n    return results\\n\\ndef create_data():\\n    return np.random.rand(1000, 1000)  # Creates a 1000x1000 matrix\\n\\nif __name__ == \'__main__\':\\n    data = create_data()\\n    process_data(data)\\n\\n# Run from the command line:\\n# kernprof -l my_script.py\\n# python -m line_profiler my_script.py.lprof\\n```\\n\\n**Explanation:**\\n1.  We install `line_profiler` using `pip install line_profiler`.\\n2.  We decorate the function we want to profile with `@profile`.  This decorator is provided by `line_profiler`.\\n3.  We define a function `process_data` that simulates some data processing tasks. The use of `numpy.sqrt` highlights how optimization using optimized libraries can significantly improve performance\\n4.  We run the script using `kernprof -l my_script.py` to generate a `.lprof` file.\\n5.  We view the profiling results using `python -m line_profiler my_script.py.lprof`.  This will print a line-by-line breakdown of the execution time.\\n\\n**Real-World Application:**\\nImagine you\'re working with a custom image processing algorithm.  `line_profiler` can help you identify which lines of code in your algorithm are taking the most time, such as a computationally intensive loop or a specific image filtering operation.  You can then focus on optimizing those lines by using vectorized operations, alternative algorithms, or Cython.\\n\\n**Best Practices:**\\n*   Use `@profile` decorator judiciously to avoid profiling unnecessary functions.\\n*   Focus on lines with high time per hit and high hit counts.\\n*   Ensure your code is run in a way that triggers the functions you\'re profiling (e.g., provide sample data).\\n*   Consider using `%lprun` magic command in IPython/Jupyter Notebook for interactive profiling.\\n*   Use smaller datasets to expedite the profiling process, then validate the findings on larger, representative datasets."\n            }\n          ]\n        },\n        {\n          "title": "[Memory management optimization]",\n          "content": "Efficient memory management is crucial for building scalable machine learning applications, especially when dealing with large datasets. Python\'s automatic garbage collection simplifies memory management, but understanding how memory is allocated and deallocated allows for significant performance improvements.",\n          "subtopics": [\n            {\n              "title": "Understanding Python\'s Memory Model",\n              "content": "Python uses a dynamic memory allocation scheme and automatic garbage collection. Every object in Python resides in memory managed by the Python memory manager.  This manager allocates memory in blocks of different sizes.\\n\\n**Key Concepts:**\\n*   **Reference Counting:**  Python\'s primary garbage collection mechanism. Each object maintains a count of how many references point to it. When the reference count drops to zero, the object\'s memory is freed.\\n*   **Cyclic Garbage Collector:**  Handles objects involved in reference cycles (e.g., A references B, B references A). These cycles can prevent reference counting from freeing memory, even if the objects are no longer used.  The cyclic garbage collector runs periodically to detect and break these cycles.\\n*   **`__slots__`:**  A mechanism to reduce memory consumption in classes by pre-declaring instance attributes, eliminating the need for a `__dict__` for each instance.  This is particularly beneficial when creating a large number of objects of the same class.\\n\\n**Example of `__slots__`:**\\n\\n```python\\nclass MyClass:\\n    __slots__ = [\'x\', \'y\'] # Pre-declare instance attributes\\n\\n    def __init__(self, x, y):\\n        self.x = x\\n        self.y = y\\n\\n# Without __slots__, each instance of MyClass would have a __dict__\\n# With __slots__, each instance only stores the values of x and y directly\\n\\nimport sys\\n\\n# Example usage showing memory savings\\nobj1 = MyClass(1, 2)\\n\\nif \'__dict__\' in dir(obj1):\\n    print(\\"Object has a __dict__ attribute\\")\\nelse:\\n    print(\\"Object does NOT have a __dict__ attribute\\")\\n\\nprint(\\"Size of the object (with __slots__):\\", sys.getsizeof(obj1))\\n\\nclass MyClassNoSlots:\\n    def __init__(self, x, y):\\n        self.x = x\\n        self.y = y\\n\\nobj2 = MyClassNoSlots(1, 2)\\nprint(\\"Size of the object (without __slots__):\\", sys.getsizeof(obj2))\\n#Note: `sys.getsizeof` is a lower bound on object memory usage; there is other overhead not captured here\\n\\n```\\n\\n**Explanation:**\\n1.  We define two classes: `MyClass` uses `__slots__`, while `MyClassNoSlots` does not.\\n2.  We create instances of both classes and check for the presence of a `__dict__` attribute.\\n3.  We print the sizes of the objects to demonstrate the memory savings achieved by using `__slots__`.\\n\\n**Real-World Application:**\\nConsider a scenario where you are building a recommendation system and need to store a large number of user profiles in memory. Using `__slots__` can significantly reduce the memory footprint of each user profile object, allowing you to store more profiles in memory and improve the system\'s performance.\\n\\n**Best Practices:**\\n*   Use `__slots__` when creating a large number of instances of a class with a fixed set of attributes.\\n*   Avoid creating unnecessary objects.\\n*   Release references to objects that are no longer needed to allow garbage collection to reclaim memory.\\n*   Use data structures that are memory-efficient (e.g., NumPy arrays, dictionaries instead of lists when appropriate).\\n*   Profile memory usage to identify potential memory leaks or excessive memory consumption. The `memory_profiler` is a useful tool for this."\n            },\n            {\n              "title": "Memoryview for Efficient Data Access",\n              "content": "`memoryview` provides a way to access the internal data of an object without copying it. This is particularly useful when working with large arrays or binary data, as it avoids the overhead of creating new copies of the data.\\n\\n**Key Concepts:**\\n*   **Zero-Copy Access:**  `memoryview` provides direct access to the underlying memory buffer of an object, without creating a copy.\\n*   **Flexible Data Access:**  `memoryview` allows you to access different parts of the data using slicing, indexing, and other operations.\\n*   **Read-Only or Read-Write Access:**  `memoryview` can be created with read-only or read-write access, depending on the underlying object.\\n\\n**Usage:**\\n\\n```python\\nimport numpy as np\\n\\n# Create a NumPy array\\narr = np.array([1, 2, 3, 4, 5])\\n\\n# Create a memoryview of the array\\nmv = memoryview(arr)\\n\\n# Access the data using slicing\\nsub_mv = mv[1:4]\\n\\n# Modify the data through the memoryview (if the original array is writable)\\nsub_mv[0] = 10\\n\\n# Print the original array (it will be modified)\\nprint(arr)\\n\\n#Example of Readonly memoryview\\nbyte_string = b\\"Hello, world!\\"\\nmemory_view = memoryview(byte_string)\\n\\n# Attempting to modify a readonly memoryview will raise an error.\\n# memory_view[0] = ord(\'J\') # Raises TypeError: cannot modify read-only memory\\n\\n```\\n\\n**Explanation:**\\n1.  We create a NumPy array `arr`.\\n2.  We create a `memoryview` of the array.\\n3.  We access a slice of the data using `mv[1:4]`. This returns another `memoryview` object, not a copy of the data.\\n4.  We modify the data through the `sub_mv` memoryview.  This modifies the original array `arr` directly because `memoryview` references the original memory.\\n\\n**Real-World Application:**\\nIn machine learning, `memoryview` can be used to efficiently access large image or video datasets without creating copies of the data. For example, you can use `memoryview` to access specific regions of an image or video frame without loading the entire frame into memory.\\n\\n**Best Practices:**\\n*   Use `memoryview` when you need to access large datasets without creating copies.\\n*   Be careful when modifying data through a `memoryview`, as it will directly modify the underlying object.\\n*   Consider the read-only or read-write nature of the underlying object when creating a `memoryview`."\n            }\n          ]\n        },\n        {\n          "title": "[Cython integration]",\n          "content": "Cython is a programming language that allows you to write C extensions for Python.  It provides a way to significantly improve the performance of Python code by compiling it to C and leveraging C\'s speed and efficiency.  This is especially useful for computationally intensive tasks in machine learning.",\n          "subtopics": [\n            {\n              "title": "Cython Basics",\n              "content": "Cython code looks like Python code, but it includes optional type declarations that allow the Cython compiler to generate optimized C code.  Cython compiles to C and then is compiled into a Python extension module.\\n\\n**Key Concepts:**\\n*   **Type Declarations:**  Specifying the types of variables, function arguments, and return values allows Cython to generate more efficient C code. This avoids the overhead of dynamic type checking in Python.\\n*   **C Integration:**  Cython allows you to directly call C functions and libraries, providing access to a wide range of optimized C code.\\n*   **Compilation:**  Cython code is compiled to C code, which is then compiled into a Python extension module. This module can be imported and used like any other Python module.\\n\\n**Usage:**\\n\\n1.  **Create a Cython file (e.g., `my_module.pyx`):**\\n\\n```cython\\n# cython: language_level=3\\ncpdef int fibonacci(int n):\\n    \\"\\"\\"Calculates the nth Fibonacci number.\\n    \\"\\"\\"\\n    if n < 2:\\n        return n\\n    else:\\n        return fibonacci(n-1) + fibonacci(n-2)\\n```\\n\\n2.  **Create a `setup.py` file:**\\n\\n```python\\nfrom setuptools import setup\\nfrom Cython.Build import cythonize\\n\\nsetup(\\n    ext_modules = cythonize(\\"my_module.pyx\\")\\n)\\n```\\n\\n3.  **Compile the Cython code:**\\n\\n```bash\\npython setup.py build_ext --inplace\\n```\\n\\n4.  **Use the compiled module in Python:**\\n\\n```python\\nimport my_module\\n\\nresult = my_module.fibonacci(10)\\nprint(result)\\n```\\n\\n**Explanation:**\\n1.  We create a Cython file `my_module.pyx` that defines a function `fibonacci` to calculate the nth Fibonacci number.  We use `cpdef` to declare the function as a C-callable function that can also be called from Python.\\n2.  We create a `setup.py` file that uses `cythonize` to compile the Cython code.\\n3.  We compile the Cython code using `python setup.py build_ext --inplace`. This creates a Python extension module named `my_module.so` (or `.pyd` on Windows) in the same directory as the `my_module.pyx` file.\\n4.  We import the compiled module in Python and use the `fibonacci` function.\\n\\n**Real-World Application:**\\nIn machine learning, Cython can be used to optimize computationally intensive tasks such as:\\n*   **Loss function calculations:** Implementing custom loss functions in Cython can significantly speed up model training.\\n*   **Gradient calculations:** Optimizing gradient calculations in neural networks can reduce training time.\\n*   **Feature engineering:** Performing complex feature transformations in Cython can improve the efficiency of data preprocessing.\\n\\n**Best Practices:**\\n*   Start by profiling your code to identify the performance bottlenecks.\\n*   Focus on optimizing the most time-consuming parts of your code with Cython.\\n*   Use type declarations to improve the performance of Cython code.\\n*   Use `cpdef` to declare functions that can be called from both C and Python.\\n*   Use `nogil` for CPU-bound tasks to release the Global Interpreter Lock (GIL) and enable true multi-threading."\n            },\n            {\n              "title": "Working with NumPy Arrays in Cython",\n              "content": "NumPy arrays are widely used in machine learning for storing and manipulating numerical data. Cython provides efficient ways to work with NumPy arrays, allowing you to access and manipulate array elements directly without the overhead of Python\'s dynamic type checking.\\n\\n**Key Concepts:**\\n*   **`cimport numpy`:**  Imports the NumPy C API, allowing you to work with NumPy arrays in Cython.\\n*   **Type Declarations:**  Declare the types of NumPy arrays using `np.ndarray[dtype, ndim=N]`, where `dtype` is the data type of the array (e.g., `np.float64`) and `N` is the number of dimensions.\\n*   **Direct Access:**  Access array elements directly using C-style indexing (e.g., `array[i, j]`).\\n\\n**Usage:**\\n\\n```cython\\n# cython: language_level=3\\nimport numpy as np\\ncimport numpy as np\\n\\ndef process_array(np.ndarray[np.float64, ndim=2] data):\\n    cdef int i, j\\n    cdef int rows = data.shape[0]\\n    cdef int cols = data.shape[1]\\n    cdef double sum_val = 0.0\\n\\n    for i in range(rows):\\n        for j in range(cols):\\n            sum_val += data[i, j]\\n\\n    return sum_val\\n\\n#Example Python usage\\nif __name__ == \'__main__\':\\n    import numpy as np\\n    data = np.random.rand(1000, 1000)\\n    result = process_array(data)\\n    print(result)\\n```\\n\\n\\n**Explanation:**\\n1.  We import the NumPy C API using `cimport numpy`.\\n2.  We declare the type of the input array `data` as `np.ndarray[np.float64, ndim=2]`, specifying that it is a 2D NumPy array with `float64` data type.\\n3.  We declare the types of the loop variables `i`, `j`, `rows`, `cols`, and `sum_val` as C types.\\n4.  We access the array elements directly using C-style indexing `data[i, j]`.\\n\\n**Real-World Application:**\\nIn machine learning, Cython can be used to optimize operations on large NumPy arrays, such as:\\n*   **Matrix multiplication:** Implementing custom matrix multiplication routines in Cython can significantly speed up linear algebra operations.\\n*   **Convolutional neural networks:** Optimizing convolutional operations in Cython can improve the performance of CNNs.\\n*   **Clustering algorithms:** Implementing distance calculations in Cython can speed up clustering algorithms.\\n\\n**Best Practices:**\\n*   Use type declarations for NumPy arrays to improve performance.\\n*   Avoid Python loops when possible and use C-style indexing for direct access to array elements.\\n*   Consider using NumPy\'s universal functions (ufuncs) for vectorized operations on arrays, as these are already highly optimized."\n            }\n          ]\n        },\n        {\n          "title": "[Vectorization strategies]",\n          "content": "Vectorization is the technique of performing operations on entire arrays of data at once, rather than iterating through individual elements. This is typically achieved using libraries like NumPy, which provide highly optimized vectorized operations. Vectorization significantly improves performance by leveraging underlying SIMD (Single Instruction, Multiple Data) instructions and avoiding Python\'s slow for loops.",\n          "subtopics": [\n            {\n              "title": "NumPy for Vectorized Operations",\n              "content": "NumPy is the fundamental library for numerical computing in Python. It provides a powerful array object and a wide range of functions for performing vectorized operations.\\n\\n**Key Concepts:**\\n*   **Arrays:**  NumPy arrays are homogeneous data structures that store elements of the same data type. They provide efficient storage and manipulation of numerical data.\\n*   **Universal Functions (UFuncs):**  NumPy provides universal functions (ufuncs) that perform element-wise operations on arrays. Ufuncs are highly optimized and can significantly improve performance compared to Python loops.\\n*   **Broadcasting:**  NumPy\'s broadcasting mechanism allows you to perform operations on arrays with different shapes, as long as their dimensions are compatible.\\n\\n**Usage:**\\n\\n```python\\nimport numpy as np\\n\\n# Create NumPy arrays\\narr1 = np.array([1, 2, 3, 4, 5])\\narr2 = np.array([6, 7, 8, 9, 10])\\n\\n# Vectorized addition\\nresult = arr1 + arr2\\nprint(result)\\n\\n# Vectorized multiplication\\nresult = arr1 * arr2\\nprint(result)\\n\\n# Vectorized square root\\nresult = np.sqrt(arr1)\\nprint(result)\\n\\n# Vectorized comparison\\nresult = arr1 > 3\\nprint(result)\\n\\n#Broadcasting examples\\nscalar = 2\\nresult = arr1 + scalar #scalar is broadcasted to match arr1\'s shape\\nprint(result)\\n\\nmatrix = np.array([[1, 2], [3, 4]])\\nvector = np.array([5, 6])\\nresult = matrix + vector #vector is broadcasted to match matrix\'s shape\\nprint(result)\\n```\\n\\n**Explanation:**\\n1.  We create two NumPy arrays `arr1` and `arr2`.\\n2.  We perform vectorized addition and multiplication using the `+` and `*` operators.\\n3.  We use the `np.sqrt()` function to calculate the square root of each element in `arr1`.\\n4.  We use the `>` operator to perform a vectorized comparison of `arr1` with 3.\\n5.  We show broadcasting examples of adding scalars and vectors to matrices.\\n\\n**Real-World Application:**\\nIn machine learning, vectorization can be used to optimize various tasks, such as:\\n*   **Data preprocessing:** Scaling, normalizing, and transforming data using vectorized operations.\\n*   **Model training:** Calculating gradients and updating model parameters using vectorized operations.\\n*   **Model prediction:** Making predictions on large datasets using vectorized operations.\\n\\n**Best Practices:**\\n*   Avoid Python loops and use NumPy\'s vectorized operations whenever possible.\\n*   Use NumPy\'s ufuncs for element-wise operations on arrays.\\n*   Leverage broadcasting to perform operations on arrays with different shapes.\\n*   Consider using NumPy\'s advanced indexing techniques for more complex array manipulations."\n            },\n            {\n              "title": "Avoiding Loops with NumPy Broadcasting and UFuncs",\n              "content": "The key to effective vectorization is to eliminate explicit loops as much as possible. NumPy\'s broadcasting and ufuncs are powerful tools for achieving this.\\n\\n**Key Concepts:**\\n*   **UFuncs as Loop Replacements:** NumPy\'s ufuncs (Universal Functions) are designed to operate element-wise on arrays, effectively replacing explicit Python loops. They\'re implemented in highly optimized C code.\\n*   **Broadcasting Rules:** NumPy\'s broadcasting allows operations between arrays with different shapes under certain rules. Understanding these rules helps write concise and efficient code.\\n\\n**Usage:**\\n\\n```python\\nimport numpy as np\\n\\n# Example 1: Calculating Euclidean distance between two arrays of vectors\\ndef euclidean_distance_loop(x, y):\\n    distance = np.zeros(x.shape[0])\\n    for i in range(x.shape[0]):\\n        distance[i] = np.sqrt(np.sum((x[i, :] - y[i, :])**2))\\n    return distance\\n\\ndef euclidean_distance_vectorized(x, y):\\n    return np.sqrt(np.sum((x - y)**2, axis=1))\\n\\nx = np.random.rand(1000, 2)\\ny = np.random.rand(1000, 2)\\n\\n#Example 2: Normalizing rows of a matrix\\nmatrix = np.random.rand(5,3)\\n\\n# Without vectorization - not recommended!\\ndef normalize_rows_loop(matrix):\\n    for i in range(matrix.shape[0]):\\n        row_sum = np.sum(matrix[i,:])\\n        matrix[i, :] = matrix[i, :] / row_sum\\n    return matrix\\n\\n#With vectorization\\ndef normalize_rows_vectorized(matrix):\\n    row_sums = np.sum(matrix, axis=1, keepdims=True) #keepdims ensures shape (5,1)\\n    return matrix / row_sums\\n\\nimport time\\nstart = time.time()\\neuclidean_distance_loop(x,y)\\nend = time.time()\\nprint(f\\"Loop method: {end - start}\\")\\n\\nstart = time.time()\\neuclidean_distance_vectorized(x,y)\\nend = time.time()\\nprint(f\\"Vectorized method: {end - start}\\")\\n\\n\\n```\\n\\n**Explanation:**\\n1. In `euclidean_distance_vectorized`, broadcasting is used to perform element-wise subtraction between corresponding rows of `x` and `y`, followed by squaring and summing along the columns (`axis=1`). The square root is then applied element-wise.\\n2.  In `normalize_rows_vectorized`, `np.sum` calculates the sum of each row. The `keepdims=True` argument ensures that `row_sums` has the shape `(5, 1)`, allowing for broadcasting during division.\\n\\n**Real-World Application:**\\nIn machine learning, these techniques are applicable to various tasks, such as:\\n*   **K-Nearest Neighbors:** Efficiently computing distances between data points without loops.\\n*   **Neural Network Layers:** Normalizing inputs to layers to improve training stability and speed.\\n\\n**Best Practices:**\\n*   Understand NumPy\'s broadcasting rules to avoid unexpected behavior and ensure correct calculations.\\n*   Use ufuncs for element-wise operations whenever possible.\\n*   Profile your code to identify opportunities for vectorization."\n            }\n          ]\n        },\n        {\n          "title": "[Benchmarking techniques]",\n          "content": "Benchmarking is the process of measuring the performance of your code to compare different implementations or identify performance bottlenecks. Accurate benchmarking is crucial for validating optimizations and ensuring that your code meets performance requirements. We will cover a variety of benchmarking tools that can be applied.",\n          "subtopics": [\n            {\n              "title": "`timeit` module",\n              "content": "`timeit` is a Python module that provides a simple and accurate way to measure the execution time of small code snippets. It\'s designed to avoid common pitfalls in timing code, such as measuring startup overhead or being affected by garbage collection.\\n\\n**Key Concepts:**\\n*   **Repetitions:**  `timeit` runs the code snippet multiple times to reduce the impact of variations in system performance.\\n*   **Timer:**  `timeit` uses a precise timer to measure the execution time of the code snippet.\\n*   **Setup Code:**  `timeit` allows you to specify setup code that is executed before each repetition of the code snippet. This is useful for initializing data or importing modules.\\n\\n**Usage:**\\n\\n```python\\nimport timeit\\n\\n# Code snippet to benchmark\\ncode_to_benchmark = \\"\\"\\"\\nimport numpy as np\\narr = np.random.rand(1000)\\nresult = np.sum(arr)\\n\\"\\"\\"\\n\\n# Setup code (executed once before timing)\\nsetup_code = \\"\\"\\"\\nimport numpy as np\\n\\"\\"\\"\\n\\n# Benchmark the code snippet\\nnum_repetitions = 1000\\nexecution_time = timeit.timeit(code_to_benchmark, setup=setup_code, number=num_repetitions)\\n\\nprint(f\\"Execution time: {execution_time / num_repetitions:.6f} seconds (averaged over {num_repetitions} repetitions)\\")\\n\\n#Alternative: use the timeit command line tool.\\n#Example: python -m timeit -n 1000 \'import numpy as np; arr = np.random.rand(1000); result = np.sum(arr)\'\\n\\n#Another example showing a performance comparison.\\n\\ndef list_comprehension():\\n    return [i**2 for i in range(1000)]\\n\\ndef loop_approach():\\n    result = []\\n    for i in range(1000):\\n        result.append(i**2)\\n    return result\\n\\n\\ntime_list_comprehension = timeit.timeit(list_comprehension, number=1000)\\ntime_loop_approach = timeit.timeit(loop_approach, number=1000)\\n\\nprint(f\\"List Comprehension: {time_list_comprehension/1000} seconds\\")\\nprint(f\\"Loop approach: {time_loop_approach/1000} seconds\\")\\n```\\n\\n**Explanation:**\\n1.  We import the `timeit` module.\\n2.  We define the code snippet to be benchmarked in the `code_to_benchmark` variable.\\n3.  We define the setup code in the `setup_code` variable.\\n4.  We use `timeit.timeit` to benchmark the code snippet, specifying the setup code, the number of repetitions, and the number of times to run the code snippet in each repetition.\\n5.  We print the execution time, averaged over all repetitions.\\n\\n**Real-World Application:**\\nIn machine learning, `timeit` can be used to benchmark different implementations of a function or algorithm to determine which is the most efficient. For example, you can use `timeit` to compare the performance of different data preprocessing techniques or different implementations of a machine learning model.\\n\\n**Best Practices:**\\n*   Use a sufficient number of repetitions to reduce the impact of variations in system performance.\\n*   Specify setup code to initialize data or import modules.\\n*   Avoid measuring code that includes startup overhead or garbage collection.\\n*   Run benchmarks on a consistent environment to ensure fair comparisons."\n            },\n            {\n              "title": "`perf` (Linux profiler)",\n              "content": "`perf` is a powerful performance analysis tool available on Linux systems. It allows you to collect detailed performance data, such as CPU cycles, cache misses, and branch predictions. `perf` is a system-wide profiler, meaning it can profile any process running on the system.\\n\\n**Key Concepts:**\\n*   **Hardware Performance Counters:**  `perf` uses hardware performance counters to collect detailed performance data.\\n*   **Sampling:**  `perf` samples the program\'s execution at regular intervals to collect performance data.\\n*   **Call Graph:**  `perf` can generate a call graph showing the relationships between functions and their execution times.\\n\\n**Usage:**\\n\\n```bash\\n# Install perf (if not already installed)\\nsudo apt-get install linux-tools-common linux-tools-$(uname -r)\\nsudo apt-get install linux-perf\\n\\n#Example Python code: sample_script.py\\n#Example Code\\nimport numpy as np\\n\\ndef slow_function():\\n    result = 0\\n    for i in range(1000000):\\n        result += i\\n    return result\\n\\n\\ndef another_slow_function():\\n    result = 0\\n    for i in range(500000):\\n        result *= (i + 1) if (i+1) % 2 == 0 else 1 # Simulate different complexity\\n    return result\\n\\n\\ndef main_function():\\n    slow_function()\\n    another_slow_function()\\n\\nmain_function()\\n\\n# Collect performance data\\nsudo perf record -g python sample_script.py\\n\\n# Analyze the performance data\\nperf report\\n```\\n\\n**Explanation:**\\n1.  We install `perf` if it\'s not already installed.\\n2.  We use `perf record -g python sample_script.py` to collect performance data while running the Python script `sample_script.py`. The `-g` option tells `perf` to record call graph information.\\n3.  We use `perf report` to analyze the collected performance data. This will display a report showing the functions that consumed the most CPU time.\\n\\n**Real-World Application:**\\nIn machine learning, `perf` can be used to identify performance bottlenecks in your code, such as:\\n*   **Slow functions:** Identifying functions that consume a significant amount of CPU time.\\n*   **Cache misses:** Identifying code that causes a high number of cache misses.\\n*   **Branch mispredictions:** Identifying code that causes a high number of branch mispredictions.\\n\\n**Best Practices:**\\n*   Run `perf` with root privileges to access hardware performance counters.\\n*   Use the `-g` option to record call graph information.\\n*   Analyze the performance data to identify performance bottlenecks.\\n*   Use the `perf annotate` command to view the source code with performance annotations.\\n\\n**Security Considerations:** Running `perf',
        },
      ],
    },
  ],
});
